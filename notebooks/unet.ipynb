{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faisal/anaconda3/envs/ai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread\n",
    "\n",
    "from gen.load_data import load_data\n",
    "\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                image                   id  \\\n",
      "1143  ../data/Train/CameraRGB/episode_0001_000087.png  episode_0001_000087   \n",
      "1878                  ../data/Train/CameraRGB/355.png                  355   \n",
      "1747                  ../data/Train/CameraRGB/460.png                  460   \n",
      "1221  ../data/Train/CameraRGB/episode_0003_000141.png  episode_0003_000141   \n",
      "2407  ../data/Train/CameraRGB/episode_0003_000130.png  episode_0003_000130   \n",
      "\n",
      "                                                label  \n",
      "1143  ../data/Train/CameraSeg/episode_0001_000087.png  \n",
      "1878                  ../data/Train/CameraSeg/355.png  \n",
      "1747                  ../data/Train/CameraSeg/460.png  \n",
      "1221  ../data/Train/CameraSeg/episode_0003_000141.png  \n",
      "2407  ../data/Train/CameraSeg/episode_0003_000130.png  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_df, valid_df, test_df = load_data('../data')\n",
    "\n",
    "train_df = shuffle(train_df)\n",
    "valid_df = shuffle(valid_df)\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 480, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 480, 480, 3)  12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 480, 480, 3)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 480, 480, 8)  224         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 480, 480, 8)  584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 240, 240, 8)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 240, 240, 16) 1168        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 240, 240, 16) 2320        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 120, 120, 16) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 120, 120, 32) 4640        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 120, 120, 32) 9248        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 60, 60, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 60, 60, 64)   18496       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 60, 60, 64)   36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 30, 30, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 30, 30, 128)  73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 30, 30, 128)  147584      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 60, 60, 64)   32832       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 60, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 60, 60, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 60, 60, 64)   36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 120, 120, 32) 8224        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 120, 120, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 120, 120, 32) 18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 120, 120, 32) 9248        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 240, 240, 16) 2064        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 240, 240, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 240, 240, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 240, 240, 16) 2320        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 480, 480, 8)  520         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 480, 480, 16) 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 480, 480, 8)  1160        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 480, 480, 8)  584         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 480, 480, 3)  27          conv2d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 485,847\n",
      "Trainable params: 485,841\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models.unet import model_unet\n",
    "\n",
    "model = model_unet(3, image_shape=(480, 480, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gen.generators import train_and_lab_gen_func\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "model_dir = '../saved_models/unet/'\n",
    "\n",
    "train_gen = train_and_lab_gen_func(train_df, image_size=(600, 800), target_size=(480, 480), batch_size = BATCH_SIZE)\n",
    "valid_gen = train_and_lab_gen_func(valid_df, image_size=(600, 800), target_size=(480, 480), batch_size = BATCH_SIZE)\n",
    "# test_gen = train_and_lab_gen_func(test_df, image_size=(600, 800),  target_size=(480, 480),  batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from train import jaccard_distance_loss\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "gpus = 2\n",
    "\n",
    "model = multi_gpu_model(model, gpus)\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "                   loss = jaccard_distance_loss, \n",
    "                   metrics = ['acc', 'categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faisal/anaconda3/envs/ai/lib/python3.6/site-packages/keras/engine/training.py:2095: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Reinserting dataframe: 500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Reinserting dataframe: 500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 500 images\n",
      "Reinserting dataframe: 2500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 500 images\n",
      "Reinserting dataframe: 2500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Reinserting dataframe: 500 images\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 500 images\n",
      "Reinserting dataframe: 2500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Reinserting dataframe: 2500 images\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 2500 images\n",
      "Epoch 1/100\n",
      "Reinserting dataframe: 2500 images\n",
      "Reinserting dataframe: 2500 images\n",
      "Reinserting dataframe: 2500 images\n",
      "39/39 [==============================] - 122s 3s/step - loss: 0.4430 - acc: 0.7740 - categorical_accuracy: 0.7740 - val_loss: 0.4404 - val_acc: 0.7753 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44039, saving model to ../saved_models/unet//model.hdf5\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - 95s 2s/step - loss: 0.4403 - acc: 0.7754 - categorical_accuracy: 0.7754 - val_loss: 0.4363 - val_acc: 0.7774 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44039 to 0.43630, saving model to ../saved_models/unet//model.hdf5\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - 96s 2s/step - loss: 0.4410 - acc: 0.7750 - categorical_accuracy: 0.7750 - val_loss: 0.4413 - val_acc: 0.7749 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - 95s 2s/step - loss: 0.4427 - acc: 0.7741 - categorical_accuracy: 0.7741 - val_loss: 0.4444 - val_acc: 0.7733 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - 99s 3s/step - loss: 0.4385 - acc: 0.7763 - categorical_accuracy: 0.7763 - val_loss: 0.4441 - val_acc: 0.7734 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - 98s 3s/step - loss: 0.4439 - acc: 0.7736 - categorical_accuracy: 0.7736 - val_loss: 0.4358 - val_acc: 0.7777 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43630 to 0.43584, saving model to ../saved_models/unet//model.hdf5\n",
      "Epoch 7/100\n",
      "39/39 [==============================] - 96s 2s/step - loss: 0.4419 - acc: 0.7745 - categorical_accuracy: 0.7745 - val_loss: 0.4446 - val_acc: 0.7732 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - 93s 2s/step - loss: 0.4409 - acc: 0.7751 - categorical_accuracy: 0.7751 - val_loss: 0.4355 - val_acc: 0.7778 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.43584 to 0.43554, saving model to ../saved_models/unet//model.hdf5\n",
      "Epoch 9/100\n",
      "39/39 [==============================] - 92s 2s/step - loss: 0.4408 - acc: 0.7751 - categorical_accuracy: 0.7751 - val_loss: 0.4496 - val_acc: 0.7706 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "39/39 [==============================] - 94s 2s/step - loss: 0.4401 - acc: 0.7755 - categorical_accuracy: 0.7755 - val_loss: 0.4450 - val_acc: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - 100s 3s/step - loss: 0.4375 - acc: 0.7768 - categorical_accuracy: 0.7768 - val_loss: 0.4439 - val_acc: 0.7735 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - 94s 2s/step - loss: 0.4430 - acc: 0.7740 - categorical_accuracy: 0.7740 - val_loss: 0.4433 - val_acc: 0.7739 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - 99s 3s/step - loss: 0.4424 - acc: 0.7743 - categorical_accuracy: 0.7743 - val_loss: 0.4407 - val_acc: 0.7752 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - 97s 2s/step - loss: 0.4407 - acc: 0.7752 - categorical_accuracy: 0.7752 - val_loss: 0.4467 - val_acc: 0.7721 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "19/39 [=============>................] - ETA: 44s - loss: 0.4410 - acc: 0.7750 - categorical_accuracy: 0.7750"
     ]
    }
   ],
   "source": [
    "from train import train_nn\n",
    "\n",
    "m = train_df.shape[0]\n",
    "history = train_nn(model, \n",
    "                   train_gen, \n",
    "                   valid_gen, \n",
    "                   training_size=m, \n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   validation_size=valid_df.shape[0],\n",
    "                   output_path=model_dir, \n",
    "                   epochs=100,\n",
    "                  gpus = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('{}/model_saved.h5'.format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
